{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b5a9e27d-5e22-4a5c-aefe-4df370240166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing videos: ████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████\n",
      "\n",
      "✅ All video data saved to /Users/parker.pape/Projects/Completion Rate Decline Analysis/All Video Data.csv\n",
      "✅ Elapsed Time: 5524.31 seconds\n",
      "Count: 808 videos\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import re\n",
    "import textstat\n",
    "import subprocess\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import detect_silence as silence\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "from urllib.request import urlretrieve\n",
    "import tempfile\n",
    "from nltk.tokenize import sent_tokenize as nltk_sent_tokenize\n",
    "from deepmultilingualpunctuation import PunctuationModel\n",
    "from transformers import pipeline\n",
    "import warnings\n",
    "import contextlib\n",
    "import io\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "subprocess.run(\n",
    "    [sys.executable, '-c', 'from deepmultilingualpunctuation import PunctuationModel; PunctuationModel()'],\n",
    "    stdout=subprocess.DEVNULL,\n",
    "    stderr=subprocess.DEVNULL\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"`grouped_entities` is deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"`Device set to use mps:0\")\n",
    "\n",
    "# Tokenizers\n",
    "naive_sent_tokenize = lambda text: re.split(r'(?<=[.!?])\\s+', text)\n",
    "naive_word_tokenize = lambda text: re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "\n",
    "def load_amplitude_data(pathway):\n",
    "    with open(pathway, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    file_name = lines[0].strip().strip('\"')\n",
    "    csv_content = '\\n'.join(lines[3:])\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_csv(StringIO(csv_content), delimiter='\\t')\n",
    "\n",
    "    # Clean column names\n",
    "    df.columns = (\n",
    "        df.columns\n",
    "        .str.strip()\n",
    "        .str.replace(r'[\"\\']', '', regex=True)\n",
    "        .str.replace(r'[,\\s]+$', '', regex=True)\n",
    "    )\n",
    "\n",
    "    # Fix trailing commas and quotes in numeric columns\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            if df[col].str.contains(r'^\\s*\\d+(?:\\.\\d+)?[\"\\']?,\\s*$', regex=True).any():\n",
    "                df[col] = df[col].str.replace(r'[\"\\']', '', regex=True)\n",
    "                df[col] = df[col].str.rstrip(',')\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Strip trailing commas, quotes, and whitespace from string-like cells\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == object:\n",
    "            df[col] = df[col].astype(str).str.strip().str.replace(r'[\"\\',]+$', '', regex=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def combine_amplitude_data(df, new_pathway):\n",
    "    with open(new_pathway, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    file_name = lines[0].strip().strip('\"')\n",
    "    csv_content = '\\n'.join(lines[3:])\n",
    "\n",
    "    # Load new data\n",
    "    new_df = pd.read_csv(StringIO(csv_content), delimiter='\\t')\n",
    "\n",
    "    # Clean column names\n",
    "    new_df.columns = (\n",
    "        new_df.columns\n",
    "        .str.strip()\n",
    "        .str.replace(r'[\"\\']', '', regex=True)\n",
    "        .str.replace(r'[,\\s]+$', '', regex=True)\n",
    "    )\n",
    "\n",
    "    # Clean and normalize CMS Urls in both dataframes\n",
    "    df['CMS Url'] = df['CMS Url'].astype(str).str.strip()\n",
    "    new_df['CMS Url'] = new_df['CMS Url'].astype(str).str.strip()\n",
    "\n",
    "    # Fix numeric columns in new_df\n",
    "    for col in new_df.columns:\n",
    "        if new_df[col].dtype == 'object':\n",
    "            new_df[col] = new_df[col].astype(str).str.strip().str.replace(r'[\"\\',]+$', '', regex=True)\n",
    "            if new_df[col].str.contains(r'^\\s*\\d+(?:\\.\\d+)?[\"\\']?,?\\s*$', regex=True).any():\n",
    "                new_df[col] = pd.to_numeric(new_df[col], errors='coerce')\n",
    "\n",
    "    # Only bring over the Watched Ads columns\n",
    "    watched_ads_cols = [col for col in new_df.columns if 'Watched Ads' in col]\n",
    "    columns_to_merge = ['CMS Url'] + watched_ads_cols\n",
    "\n",
    "    # Merge on cleaned CMS Urls\n",
    "    merged_df = df.merge(new_df[columns_to_merge], on='CMS Url', how='left')\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "def extract_or_pass(val):\n",
    "    hyperlink_pattern = r'HYPERLINK\\(\"([^\"]+)\"'\n",
    "    if isinstance(val, str):\n",
    "        match = re.search(hyperlink_pattern, val)\n",
    "        if match:\n",
    "            return match.group(1)  # return the extracted URL\n",
    "    return val  # return original if no match\n",
    "\n",
    "def add_cms_data(df):\n",
    "    # Extract UUID from CMS URL\n",
    "    df['UUID'] = df['CMS Url'].str.extract(r'uuid=([a-f0-9\\-]+)', expand=False).str.lower().str.strip()\n",
    "    cms_df['UUID'] = cms_df['UUID'].astype(str).str.lower().str.strip()\n",
    "\n",
    "    # Merge CMS data into main DataFrame\n",
    "    merged_df = df.merge(cms_df[['UUID', 'Live Link', 'Publish Date']], on='UUID', how='left')\n",
    "\n",
    "    # Convert Publish Date to datetime and extract components\n",
    "    publish_datetime = pd.to_datetime(merged_df['Publish Date'], errors='coerce')\n",
    "    merged_df['Publish Date'] = publish_datetime.dt.strftime('%Y-%m-%d')\n",
    "    merged_df['Publish Hour'] = publish_datetime.dt.hour\n",
    "    merged_df['Day of Week'] = publish_datetime.dt.day_name()\n",
    "    merged_df['Month'] = publish_datetime.dt.month\n",
    "\n",
    "    # Extract collection\n",
    "    merged_df['Collection'] = merged_df['Live Link'].str.extract(r'https?://weather\\.com/([^/]+)/', expand=False)\n",
    "\n",
    "    # Convert necessary columns to numeric\n",
    "    cols_to_convert = ['Videos Played--All Users', 'Watched Ads--All Users', 'Completion Rate--All Users']\n",
    "    for col in cols_to_convert:\n",
    "        if col in merged_df.columns:\n",
    "            merged_df[col] = pd.to_numeric(merged_df[col], errors='coerce')\n",
    "        else:\n",
    "            merged_df[col] = None\n",
    "\n",
    "    # ✅ Compute monthly averages for normalization\n",
    "    monthly_averages = (\n",
    "        merged_df.groupby('Month')[cols_to_convert]\n",
    "        .mean()\n",
    "        .to_dict('index')\n",
    "    )\n",
    "\n",
    "    # ✅ Define scoring function INSIDE so it can see monthly_averages\n",
    "    def calculate_content_score(row):\n",
    "        month = row['Month']\n",
    "        if month not in monthly_averages:\n",
    "            return None\n",
    "\n",
    "        avg = monthly_averages[month]\n",
    "\n",
    "        try:\n",
    "            norm_videos = row['Videos Played--All Users'] / avg['Videos Played--All Users']\n",
    "            norm_ads = row['Watched Ads--All Users'] / avg['Watched Ads--All Users']\n",
    "            norm_cr = row['Completion Rate--All Users'] / avg['Completion Rate--All Users']\n",
    "        except ZeroDivisionError:\n",
    "            return None\n",
    "\n",
    "        # Weighted normalized score\n",
    "        if month == 3:\n",
    "            return norm_videos * 0.4047 + norm_ads * 0.3999 + norm_cr * 0.2\n",
    "        elif month == 4:\n",
    "            return norm_videos * 0.4009 + norm_ads * 0.3963 + norm_cr * 0.2074\n",
    "        elif month == 5:\n",
    "            return norm_videos * 0.3971 + norm_ads * 0.3928 + norm_cr * 0.2147\n",
    "        elif month == 6:\n",
    "            return norm_videos * 0.39331 + norm_ads * 0.3892 + norm_cr * 0.2221\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # Apply the scoring function\n",
    "    merged_df['Content Score'] = merged_df.apply(calculate_content_score, axis=1)\n",
    "\n",
    "    # Get rid of Month\n",
    "    merged_df.drop(columns=['Month'], inplace=True)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "def add_video_metadata(df):\n",
    "    global progress_bar\n",
    "\n",
    "    df = df[df['Live Link'].notna() & df['Live Link'].str.startswith('http')]\n",
    "\n",
    "    durations = []\n",
    "    word_counts = []\n",
    "    avg_sentence_length = []\n",
    "    transcript_scores = []\n",
    "    cut_counts = []\n",
    "    avg_silences = []\n",
    "    mp4_urls = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        url = row['Live Link']\n",
    "        \n",
    "        video_duration = None\n",
    "        word_count = None\n",
    "        avg_sentence_len = None\n",
    "        readability = None\n",
    "        cut_count = None\n",
    "        avg_silence = None\n",
    "        mp4_url = None\n",
    "\n",
    "        try:\n",
    "            resp = requests.get(url, timeout=10)\n",
    "            html_text = resp.text\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Duration\n",
    "        match = re.search(r'\"duration\"\\s*:\\s*\"((\\d{1,2}:)?\\d{1,2}:\\d{2})\"', html_text)\n",
    "        if match:\n",
    "            duration_str = match.group(1)\n",
    "            time_parts = list(map(int, duration_str.split(\":\")))\n",
    "        \n",
    "            if len(time_parts) == 3:\n",
    "                h, m, s = time_parts\n",
    "            elif len(time_parts) == 2:\n",
    "                h = 0\n",
    "                m, s = time_parts\n",
    "            elif len(time_parts) == 1:\n",
    "                h, m, s = 0, 0, time_parts[0]\n",
    "            else:\n",
    "                h = m = s = 0\n",
    "        \n",
    "            video_duration = h * 3600 + m * 60 + s\n",
    "\n",
    "        # Transcript\n",
    "        transcript_match = re.search(r'\"transcript\"\\s*:\\s*\"(.+?)\"', html_text)\n",
    "        if transcript_match:\n",
    "            raw_transcript = transcript_match.group(1)\n",
    "\n",
    "            # Convert raw transcript to punctuated transcript\n",
    "            transcript = model.restore_punctuation(raw_transcript)\n",
    "            \n",
    "            # Tokenize and count\n",
    "            words = naive_word_tokenize(transcript)\n",
    "            sentences = naive_sent_tokenize(transcript)\n",
    "\n",
    "            # Word Count and Average Sentence Length\n",
    "            word_count = len(words)\n",
    "            sentence_count = len(sentences)\n",
    "            avg_sentence_len = word_count / sentence_count\n",
    "\n",
    "            # Readability Scores\n",
    "            readability = textstat.flesch_kincaid_grade(transcript)\n",
    "\n",
    "        # MP4\n",
    "        mp4_match = re.search(r'https?://[^\"]+\\.mp4', html_text)\n",
    "        if mp4_match:\n",
    "            mp4_url = mp4_match.group(0)\n",
    "            mp4_urls.append(mp4_url)\n",
    "        \n",
    "            os.makedirs(\"audio_files\", exist_ok=True)\n",
    "            uuid = str(row['UUID']).strip()\n",
    "            output_audio = os.path.join(\"audio_files\", f\"audio_{uuid}.wav\")\n",
    "        \n",
    "            try:\n",
    "                # Download mp4 to temp file\n",
    "                with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False) as tmp_file:\n",
    "                    urlretrieve(mp4_url, tmp_file.name)\n",
    "                    local_mp4_path = tmp_file.name\n",
    "        \n",
    "                # Convert to wav for silence detection\n",
    "                subprocess.run([\n",
    "                    \"ffmpeg\", \"-y\", \"-i\", local_mp4_path,\n",
    "                    \"-vn\", \"-acodec\", \"pcm_s16le\", \"-ar\", \"44100\", \"-ac\", \"2\",\n",
    "                    output_audio\n",
    "                ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)\n",
    "        \n",
    "                # Run ffprobe locally\n",
    "                ffprobe_cmd = (\n",
    "                    f'ffprobe -show_frames -of compact=p=0 '\n",
    "                    + r'-f lavfi \"movie=' + local_mp4_path + r',select=gt(scene\\,0.3)\"'\n",
    "                )\n",
    "        \n",
    "                result = subprocess.run(\n",
    "                    ffprobe_cmd, shell=True,\n",
    "                    stdout=subprocess.PIPE, stderr=subprocess.DEVNULL,\n",
    "                    text=True\n",
    "                )\n",
    "        \n",
    "                scene_lines = result.stdout.strip().split('\\n')\n",
    "                scene_changes = [line for line in scene_lines if 'media_type=video' in line]\n",
    "                cut_count = len(scene_changes)\n",
    "        \n",
    "            except Exception as e:\n",
    "                pass\n",
    "        \n",
    "            try:\n",
    "                audio = AudioSegment.from_file(output_audio, format=\"wav\")\n",
    "                silence_chunks = silence(\n",
    "                    audio,\n",
    "                    min_silence_len=100,\n",
    "                    silence_thresh=audio.dBFS - 16\n",
    "                )\n",
    "                durations_list = [(end - start) / 1000 for start, end in silence_chunks]\n",
    "                avg_silence = sum(durations_list) / len(durations_list) if durations_list else 0\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "            finally:\n",
    "                # Clean up temp mp4 file\n",
    "                try:\n",
    "                    os.remove(local_mp4_path)\n",
    "                except Exception:\n",
    "                    pass\n",
    "        \n",
    "        else:\n",
    "            mp4_urls.append(None)\n",
    "\n",
    "        durations.append(video_duration)\n",
    "        word_counts.append(word_count)\n",
    "        transcript_scores.append(readability)\n",
    "        avg_sentence_length.append(avg_sentence_len)\n",
    "        cut_counts.append(cut_count) \n",
    "        avg_silences.append(avg_silence)\n",
    "\n",
    "        progress_bar += \"█\"\n",
    "        print(f\"\\rProcessing videos: {progress_bar}\", end=\"\", flush=True)\n",
    "    \n",
    "    df = df.reset_index(drop=True)\n",
    "    df['Video Length'] = durations\n",
    "    df['Word Count'] = word_counts\n",
    "    df['WPM'] = df['Word Count'] / (df['Video Length'] / 60)\n",
    "    df['Readibility Score'] = transcript_scores\n",
    "    df['Average Sentence Length'] = avg_sentence_length\n",
    "    df['Number of Shots Changes'] = cut_counts\n",
    "    df['Average Silence Length'] = avg_silences # average length of a silence chunk\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "cms_df = pd.read_csv('/Users/parker.pape/Downloads/CMS Export - Sheet 1.csv')\n",
    "amplitude_pathway = '/Users/parker.pape/Projects/Completion Rate Decline Analysis/Data Table - Comparing CR to Videos Played.csv'\n",
    "new_pathway = '/Users/parker.pape/Projects/Completion Rate Decline Analysis/Data Table - Comparing CR to Watched Ads.csv'\n",
    "output_pathway = '/Users/parker.pape/Projects/Completion Rate Decline Analysis/All Video Data.csv'\n",
    "\n",
    "video_data = load_amplitude_data(amplitude_pathway) # Completion Rates and View Counts by Platform\n",
    "\n",
    "video_data = combine_amplitude_data(video_data, new_pathway) # Ads Watched by Platform\n",
    "\n",
    "video_data = add_cms_data(video_data) # Live Link, Publish Date, Publish Hour, Collection, Content Score\n",
    "\n",
    "print(\"Processing videos:\", end=\"\", flush=True)\n",
    "progress_bar = \"\"\n",
    "\n",
    "video_data = add_video_metadata(video_data) # Video Length, Word Count, WPM, Readability Score, \n",
    "                                            # Avg Sentence Length, Number of Shots/Clips, Avg Silence Length\n",
    "\n",
    "video_data_clean = video_data.dropna(axis=1, how='all')\n",
    "\n",
    "# Output to csv\n",
    "video_data_clean.to_csv(output_pathway, index=False)\n",
    "print(f\"\\n\\n✅ All video data saved to {output_pathway}\")\n",
    "\n",
    "# Calculate time and number of videos\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"✅ Elapsed Time: {elapsed_time:.2f} seconds\")\n",
    "print(f\"Count: {len(video_data_clean)} videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40e7992-b25c-409a-8dae-8cf798aa0049",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
